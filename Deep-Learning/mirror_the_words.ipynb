{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "题目：Problem 3（keras解法）\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.下载text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000, ' collectively and that goods be distributed by need not labor an early anarchist communist was joseph d jacque the first person to describe himself as libertarian unlike proudhon he argued that it is ')\n",
      "(10000, ' anarchism originated as a term of abuse first used against early working class radicals including t')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 10000\n",
    "train_size = 2000000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:valid_size+train_size]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:200])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "# 将字符转为id\n",
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ylevitcelloc dna aht\n",
      "nr redrob htiw elihc \n"
     ]
    }
   ],
   "source": [
    "def mirror(text):\n",
    "    words = text.split(\" \")\n",
    "    mirror = []\n",
    "    for word in words:\n",
    "        mirror.append(word[::-1])\n",
    "    return \" \".join(mirror)\n",
    "\n",
    "print(mirror(\" collectively and tha\"))\n",
    "print(mirror(\"rn border with chile \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "\n",
    "def build_dataset(text):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    length = len(text) / seq_len\n",
    "    for i in range(length):\n",
    "        line = text[i:i+seq_len]\n",
    "        mirror_line = mirror(line)\n",
    "        dataset.append([char2id(ch) for ch in line])\n",
    "        labels.append([char2id(ch) for ch in mirror_line])\n",
    "    return dataset, labels\n",
    "\n",
    "train_set, train_labels = build_dataset(train_text)\n",
    "valid_set, valid_labels = build_dataset(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 3, 15, 12, 12, 5, 3, 20, 9, 22, 5, 12, 25, 0, 1, 14, 4, 0, 20, 8], [0, 25, 12, 5, 22, 9, 20, 3, 5, 12, 12, 15, 3, 0, 4, 14, 1, 0, 8, 20])\n",
      "(' collectively and th', ' ylevitcelloc dna ht')\n"
     ]
    }
   ],
   "source": [
    "def show_string(data):\n",
    "    return \"\".join([id2char(_id) for _id in data])\n",
    "\n",
    "print(train_set[0], train_labels[0])\n",
    "print(show_string(train_set[0]), show_string(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(word, seq_len, vec_size):\n",
    "    vec = np.zeros((seq_len, vec_size), dtype=int)\n",
    "    for i, ch in enumerate(word):\n",
    "        vec[i, ch] = 1\n",
    "    return vec\n",
    "\n",
    "def vectorize_dataset(dataset, labels):\n",
    "    x = np.zeros((len(dataset), seq_len, vocabulary_size), dtype=int)\n",
    "    y = np.zeros((len(dataset), seq_len, vocabulary_size), dtype=int)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        x[i] = vectorize(dataset[i], seq_len, vocabulary_size)\n",
    "        y[i] = vectorize(labels[i], seq_len, vocabulary_size)\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = vectorize_dataset(train_set, train_labels)\n",
    "valid_x, valid_y = vectorize_dataset(valid_set, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_x:', (100000, 20, 27))\n",
      "('train_y:', (100000, 20, 27))\n",
      "('valid_x:', (500, 20, 27))\n",
      "('valid_y:', (500, 20, 27))\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y:\", train_y.shape)\n",
    "print(\"valid_x:\", valid_x.shape)\n",
    "print(\"valid_y:\", valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "\n",
    "def build_model(input_size, seq_len, hidden_size):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(input_shape=(None, input_size), units=hidden_size, return_sequences=False))\n",
    "    model.add(Dense(hidden_size, activation=\"relu\"))\n",
    "    model.add(RepeatVector(seq_len))\n",
    "    model.add(GRU(units=hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(units=input_size, activation=\"linear\")))\n",
    "    model.compile(loss=\"mse\", optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocabulary_size, seq_len, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练与评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 500 samples\n",
      "Epoch 1/128\n",
      "100000/100000 [==============================] - 190s - loss: 0.0326 - val_loss: 0.0312\n",
      "Epoch 2/128\n",
      "100000/100000 [==============================] - 182s - loss: 0.0297 - val_loss: 0.0279\n",
      "Epoch 3/128\n",
      "100000/100000 [==============================] - 170s - loss: 0.0269 - val_loss: 0.0252\n",
      "Epoch 4/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0253 - val_loss: 0.0240\n",
      "Epoch 5/128\n",
      "100000/100000 [==============================] - 167s - loss: 0.0244 - val_loss: 0.0241\n",
      "Epoch 6/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0238 - val_loss: 0.0224\n",
      "Epoch 7/128\n",
      "100000/100000 [==============================] - 163s - loss: 0.0233 - val_loss: 0.0218\n",
      "Epoch 8/128\n",
      "100000/100000 [==============================] - 161s - loss: 0.0225 - val_loss: 0.0212\n",
      "Epoch 9/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0220 - val_loss: 0.0210\n",
      "Epoch 10/128\n",
      "100000/100000 [==============================] - 167s - loss: 0.0213 - val_loss: 0.0206\n",
      "Epoch 11/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0208 - val_loss: 0.0196\n",
      "Epoch 12/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0201 - val_loss: 0.0196\n",
      "Epoch 13/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0196 - val_loss: 0.0192\n",
      "Epoch 14/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0193 - val_loss: 0.0179\n",
      "Epoch 15/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 16/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0182 - val_loss: 0.0171\n",
      "Epoch 17/128\n",
      "100000/100000 [==============================] - 170s - loss: 0.0181 - val_loss: 0.0168\n",
      "Epoch 18/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0176 - val_loss: 0.0163\n",
      "Epoch 19/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0174 - val_loss: 0.0159\n",
      "Epoch 20/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0166 - val_loss: 0.0173\n",
      "Epoch 21/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0164 - val_loss: 0.0156\n",
      "Epoch 22/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0160 - val_loss: 0.0159\n",
      "Epoch 23/128\n",
      "100000/100000 [==============================] - 163s - loss: 0.0156 - val_loss: 0.0147\n",
      "Epoch 24/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0152 - val_loss: 0.0149\n",
      "Epoch 25/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0155 - val_loss: 0.0141\n",
      "Epoch 26/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0146 - val_loss: 0.0140\n",
      "Epoch 27/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0147 - val_loss: 0.0137\n",
      "Epoch 28/128\n",
      "100000/100000 [==============================] - 163s - loss: 0.0159 - val_loss: 0.0134\n",
      "Epoch 29/128\n",
      "100000/100000 [==============================] - 163s - loss: 0.0174 - val_loss: 0.0228\n",
      "Epoch 30/128\n",
      "100000/100000 [==============================] - 161s - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 31/128\n",
      "100000/100000 [==============================] - 164s - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 32/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0139 - val_loss: 0.0127\n",
      "Epoch 33/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0133 - val_loss: 0.0123\n",
      "Epoch 34/128\n",
      "100000/100000 [==============================] - 162s - loss: 0.0141 - val_loss: 0.0121: 0\n",
      "Epoch 35/128\n",
      "100000/100000 [==============================] - 163s - loss: 0.0132 - val_loss: 0.0120\n",
      "Epoch 36/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 37/128\n",
      "100000/100000 [==============================] - 162s - loss: 0.0126 - val_loss: 0.0115\n",
      "Epoch 38/128\n",
      "100000/100000 [==============================] - 164s - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 39/128\n",
      "100000/100000 [==============================] - 162s - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 40/128\n",
      "100000/100000 [==============================] - 167s - loss: 0.0128 - val_loss: 0.0117\n",
      "Epoch 41/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0127 - val_loss: 0.0113\n",
      "Epoch 42/128\n",
      "100000/100000 [==============================] - 168s - loss: 0.0120 - val_loss: 0.0114\n",
      "Epoch 43/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0112 - val_loss: 0.0134\n",
      "Epoch 44/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 45/128\n",
      "100000/100000 [==============================] - 164s - loss: 0.0123 - val_loss: 0.0114\n",
      "Epoch 46/128\n",
      "100000/100000 [==============================] - 164s - loss: 0.0112 - val_loss: 0.0130\n",
      "Epoch 47/128\n",
      "100000/100000 [==============================] - 170s - loss: 0.0109 - val_loss: 0.0203\n",
      "Epoch 48/128\n",
      "100000/100000 [==============================] - 165s - loss: 0.0124 - val_loss: 0.0106\n",
      "Epoch 49/128\n",
      "100000/100000 [==============================] - 162s - loss: 0.0132 - val_loss: 0.0144\n",
      "Epoch 50/128\n",
      "100000/100000 [==============================] - 159s - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 51/128\n",
      "100000/100000 [==============================] - 159s - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 52/128\n",
      "100000/100000 [==============================] - 160s - loss: 0.0127 - val_loss: 0.0261\n",
      "Epoch 53/128\n",
      "100000/100000 [==============================] - 162s - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 54/128\n",
      "100000/100000 [==============================] - 158s - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 55/128\n",
      "100000/100000 [==============================] - 159s - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 56/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 57/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 58/128\n",
      "100000/100000 [==============================] - 159s - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 59/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0138 - val_loss: 0.0096\n",
      "Epoch 60/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0095 - val_loss: 0.0090\n",
      "Epoch 61/128\n",
      "100000/100000 [==============================] - 160s - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 62/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0108 - val_loss: 0.0091\n",
      "Epoch 63/128\n",
      "100000/100000 [==============================] - 158s - loss: 0.0089 - val_loss: 0.0091\n",
      "Epoch 64/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 65/128\n",
      "100000/100000 [==============================] - 161s - loss: 0.0097 - val_loss: 0.0089\n",
      "Epoch 66/128\n",
      "100000/100000 [==============================] - 159s - loss: 0.0098 - val_loss: 0.0085\n",
      "Epoch 67/128\n",
      "100000/100000 [==============================] - 158s - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 68/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0108 - val_loss: 0.0086\n",
      "Epoch 69/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 70/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0084 - val_loss: 0.0108\n",
      "Epoch 71/128\n",
      "100000/100000 [==============================] - 167s - loss: 0.0115 - val_loss: 0.0090\n",
      "Epoch 72/128\n",
      "100000/100000 [==============================] - 169s - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 73/128\n",
      "100000/100000 [==============================] - 173s - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 74/128\n",
      "100000/100000 [==============================] - 167s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 75/128\n",
      "100000/100000 [==============================] - 166s - loss: 0.0087 - val_loss: 0.0184\n",
      "Epoch 76/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 77/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0089 - val_loss: 0.0105\n",
      "Epoch 78/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 79/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0101 - val_loss: 0.0083\n",
      "Epoch 80/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0077 - val_loss: 0.0074\n",
      "Epoch 81/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0084 - val_loss: 0.0077\n",
      "Epoch 82/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 83/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0086 - val_loss: 0.0072\n",
      "Epoch 84/128\n",
      "100000/100000 [==============================] - 152s - loss: 0.0084 - val_loss: 0.0074\n",
      "Epoch 85/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0101 - val_loss: 0.0083\n",
      "Epoch 86/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 87/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0082 - val_loss: 0.0076\n",
      "Epoch 88/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 89/128\n",
      "100000/100000 [==============================] - 160s - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 90/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0092 - val_loss: 0.0076\n",
      "Epoch 91/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0075 - val_loss: 0.0122\n",
      "Epoch 92/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0078 - val_loss: 0.0071\n",
      "Epoch 93/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 94/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0072 - val_loss: 0.0121\n",
      "Epoch 95/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0081 - val_loss: 0.0072\n",
      "Epoch 96/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0108 - val_loss: 0.0158\n",
      "Epoch 97/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 98/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0078 - val_loss: 0.0127\n",
      "Epoch 99/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 100/128\n",
      "100000/100000 [==============================] - 152s - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 101/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0063 - val_loss: 0.0068\n",
      "Epoch 102/128\n",
      "100000/100000 [==============================] - 160s - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 103/128\n",
      "100000/100000 [==============================] - 171s - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 104/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 105/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0106 - val_loss: 0.0077\n",
      "Epoch 106/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 107/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0060 - val_loss: 0.0069\n",
      "Epoch 108/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0087 - val_loss: 0.0111\n",
      "Epoch 109/128\n",
      "100000/100000 [==============================] - 152s - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 110/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0083 - val_loss: 0.0196\n",
      "Epoch 111/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0089 - val_loss: 0.0064\n",
      "Epoch 112/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 113/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 114/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 115/128\n",
      "100000/100000 [==============================] - 152s - loss: 0.0066 - val_loss: 0.0061\n",
      "Epoch 116/128\n",
      "100000/100000 [==============================] - 153s - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 117/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 118/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 119/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 120/128\n",
      "100000/100000 [==============================] - 157s - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 121/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 122/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 123/128\n",
      "100000/100000 [==============================] - 156s - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 124/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 125/128\n",
      "100000/100000 [==============================] - 155s - loss: 0.0102 - val_loss: 0.0322\n",
      "Epoch 126/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 127/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0194 - val_loss: 0.0088\n",
      "Epoch 128/128\n",
      "100000/100000 [==============================] - 154s - loss: 0.0075 - val_loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a91854b50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128, \n",
    "          epochs=128,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecnenues ot ecneunes\n",
      "nr redrow htiw elihc\n",
      " ylelitcelloc dna ht\n",
      "m na nelam olluh yld\n"
     ]
    }
   ],
   "source": [
    "def test_output(test_str):\n",
    "    test_case = np.zeros((1, seq_len, vocabulary_size), dtype=int)\n",
    "    test_case[0] = vectorize([char2id(ch) for ch in test_str], seq_len, vocabulary_size)\n",
    "\n",
    "    pred = model.predict(test_case)[0]\n",
    "    print(''.join([id2char(i) for i in pred.argmax(axis=1)]))\n",
    "\n",
    "test_output(\"sequence to sequence\")\n",
    "test_output(\"rn border with chile\")\n",
    "test_output(\" collectively and th\")\n",
    "test_output(\"i am kalen hello guy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
